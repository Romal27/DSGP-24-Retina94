{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Romal27/DSGP-24-Retina94/blob/Validating-the-input/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5222 images belonging to 2 classes.\n",
            "Found 1492 images belonging to 2 classes.\n",
            "Found 748 images belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 3s/step - accuracy: 0.9392 - loss: 1.7771 - val_accuracy: 0.5442 - val_loss: 19.9984\n",
            "Epoch 2/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 3s/step - accuracy: 0.9706 - loss: 0.9057 - val_accuracy: 0.5509 - val_loss: 18.9203\n",
            "Epoch 3/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 2s/step - accuracy: 0.9772 - loss: 0.7324 - val_accuracy: 0.7882 - val_loss: 5.6246\n",
            "Epoch 4/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 2s/step - accuracy: 0.9808 - loss: 0.6411 - val_accuracy: 0.9155 - val_loss: 2.9250\n",
            "Epoch 5/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 2s/step - accuracy: 0.9848 - loss: 0.5825 - val_accuracy: 0.9350 - val_loss: 2.2543\n",
            "Epoch 6/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 2s/step - accuracy: 0.9776 - loss: 0.6140 - val_accuracy: 0.9015 - val_loss: 3.1671\n",
            "Epoch 7/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 2s/step - accuracy: 0.9800 - loss: 0.6211 - val_accuracy: 0.9403 - val_loss: 1.7575\n",
            "Epoch 8/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 2s/step - accuracy: 0.9742 - loss: 0.6756 - val_accuracy: 0.9055 - val_loss: 2.6261\n",
            "Epoch 9/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 3s/step - accuracy: 0.9862 - loss: 0.5478 - val_accuracy: 0.9350 - val_loss: 1.4005\n",
            "Epoch 10/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 4s/step - accuracy: 0.9829 - loss: 0.5213 - val_accuracy: 0.9491 - val_loss: 1.4401\n",
            "Epoch 11/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 4s/step - accuracy: 0.9835 - loss: 0.5318 - val_accuracy: 0.9705 - val_loss: 1.0213\n",
            "Epoch 12/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 4s/step - accuracy: 0.9726 - loss: 0.6922 - val_accuracy: 0.9651 - val_loss: 1.0392\n",
            "Epoch 13/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m677s\u001b[0m 4s/step - accuracy: 0.9741 - loss: 0.6336 - val_accuracy: 0.9558 - val_loss: 0.9952\n",
            "Epoch 14/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 4s/step - accuracy: 0.9866 - loss: 0.5180 - val_accuracy: 0.9517 - val_loss: 1.3759\n",
            "Epoch 15/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 4s/step - accuracy: 0.9792 - loss: 0.5432 - val_accuracy: 0.9504 - val_loss: 1.0227\n",
            "Epoch 16/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 3s/step - accuracy: 0.9761 - loss: 0.5981 - val_accuracy: 0.9578 - val_loss: 1.8112\n",
            "Epoch 17/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 3s/step - accuracy: 0.9831 - loss: 0.5688 - val_accuracy: 0.9504 - val_loss: 1.2425\n",
            "Epoch 18/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 3s/step - accuracy: 0.9815 - loss: 0.5177 - val_accuracy: 0.9638 - val_loss: 0.7684\n",
            "Epoch 19/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 3s/step - accuracy: 0.9787 - loss: 0.5021 - val_accuracy: 0.9336 - val_loss: 1.2879\n",
            "Epoch 20/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 3s/step - accuracy: 0.9795 - loss: 0.5598 - val_accuracy: 0.9645 - val_loss: 1.0502\n",
            "Epoch 21/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 3s/step - accuracy: 0.9867 - loss: 0.5104 - val_accuracy: 0.9517 - val_loss: 1.4499\n",
            "Epoch 22/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 2s/step - accuracy: 0.9838 - loss: 0.5445 - val_accuracy: 0.9786 - val_loss: 0.9092\n",
            "Epoch 23/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 2s/step - accuracy: 0.9788 - loss: 0.5282 - val_accuracy: 0.9584 - val_loss: 1.3150\n",
            "Epoch 24/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 2s/step - accuracy: 0.9861 - loss: 0.5279 - val_accuracy: 0.9786 - val_loss: 0.6780\n",
            "Epoch 25/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 2s/step - accuracy: 0.9871 - loss: 0.4613 - val_accuracy: 0.9826 - val_loss: 0.6045\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9928 - loss: 0.4598\n",
            "\n",
            "Validation Accuracy: 98.06%\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.9911 - loss: 0.4559\n",
            "\n",
            "Test Accuracy: 97.86%\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Non-Fundus       0.97      1.00      0.98       407\n",
            "      Fundus       1.00      0.96      0.98       341\n",
            "\n",
            "    accuracy                           0.98       748\n",
            "   macro avg       0.98      0.98      0.98       748\n",
            "weighted avg       0.98      0.98      0.98       748\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import uuid\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def detect_duplicates(folder):\n",
        "    seen = {}\n",
        "    duplicates = {}\n",
        "    for cls in os.listdir(folder):\n",
        "        class_path = os.path.join(folder, cls)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        duplicates[cls] = []\n",
        "        for img_name in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img_hash = hash(img.tobytes())\n",
        "                if img_hash in seen:\n",
        "                    duplicates[cls].append(img_path)\n",
        "                else:\n",
        "                    seen[img_hash] = img_path\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {e}\")\n",
        "    return duplicates\n",
        "\n",
        "\n",
        "def augment_image(image):\n",
        "    return image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "def augment_duplicates(folder, duplicates_per_class, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    for cls, duplicate_files in duplicates_per_class.items():\n",
        "        class_output_path = os.path.join(output_folder, cls)\n",
        "        os.makedirs(class_output_path, exist_ok=True)\n",
        "        for filepath in duplicate_files:\n",
        "            try:\n",
        "                with Image.open(filepath) as img:\n",
        "                    augmented_img = augment_image(img)\n",
        "                    new_filename = f\"{os.path.splitext(os.path.basename(filepath))[0]}_{uuid.uuid4().hex[:6]}.png\"\n",
        "                    new_filepath = os.path.join(class_output_path, new_filename)\n",
        "                    augmented_img.save(new_filepath)\n",
        "            except Exception as e:\n",
        "                print(f\"Error augmenting {filepath}: {e}\")\n",
        "                \n",
        "def split_data(source_dir, output_dir, train_ratio=0.7, val_ratio=0.2):\n",
        "    test_ratio = 1 - (train_ratio + val_ratio)\n",
        "    for cls in os.listdir(source_dir):\n",
        "        class_path = os.path.join(source_dir, cls)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        images = os.listdir(class_path)\n",
        "        train, temp = train_test_split(images, test_size=(1 - train_ratio), stratify=[cls]*len(images), random_state=42)\n",
        "        val, test = train_test_split(temp, test_size=(test_ratio / (val_ratio + test_ratio)), stratify=[cls]*len(temp), random_state=42)\n",
        "        for subset, subset_images in zip([\"train\", \"val\", \"test\"], [train, val, test]):\n",
        "            subset_path = os.path.join(output_dir, subset, cls)\n",
        "            os.makedirs(subset_path, exist_ok=True)\n",
        "            for img_name in subset_images:\n",
        "                shutil.copy(os.path.join(class_path, img_name), os.path.join(subset_path, img_name))\n",
        "\n",
        "# Define Paths\n",
        "dataset_path = \"D:/Datasets_Retina\"\n",
        "augmented_dataset_path = \"D:/Datasets_Retina_Augmented\"\n",
        "combined_dataset_path = \"D:/Datasets_Combined\"\n",
        "final_dataset_path = \"D:/Datasets_Final\"\n",
        "\n",
        "# Handle duplicates\n",
        "duplicates_per_class = detect_duplicates(dataset_path)\n",
        "augment_duplicates(dataset_path, duplicates_per_class, augmented_dataset_path)\n",
        "\n",
        "# Combine original and augmented datasets\n",
        "shutil.copytree(dataset_path, combined_dataset_path, dirs_exist_ok=True)\n",
        "shutil.copytree(augmented_dataset_path, combined_dataset_path, dirs_exist_ok=True)\n",
        "\n",
        "# Split dataset properly to prevent data leakage\n",
        "split_data(combined_dataset_path, final_dataset_path)\n",
        "\n",
        "# Image Processing Parameters\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data Augmentation for training set\n",
        "data_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.2,\n",
        "    brightness_range=[0.7, 1.3]\n",
        ")\n",
        "\n",
        "# Load Data\n",
        "train_generator = data_gen.flow_from_directory(os.path.join(final_dataset_path, \"train\"), target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"binary\")\n",
        "val_generator = data_gen.flow_from_directory(os.path.join(final_dataset_path, \"val\"), target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"binary\", shuffle=False)\n",
        "test_generator = data_gen.flow_from_directory(os.path.join(final_dataset_path, \"test\"), target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"binary\", shuffle=False)\n",
        "\n",
        "# Improved CNN Model with Regularization\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=(224, 224, 3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.5),\n",
        "    \n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile Model with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "\n",
        "# Train Model\n",
        "model.fit(train_generator, validation_data=val_generator, epochs=25, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate Model\n",
        "val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "print(f\"\\nValidation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predictions\n",
        "y_true = test_generator.classes\n",
        "y_pred = (model.predict(test_generator) > 0.5).astype(int)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Non-Fundus\", \"Fundus\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Fold 1/5...\n",
            "\n",
            "Found 4177 validated image filenames belonging to 2 classes.\n",
            "Found 1045 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 5s/step - accuracy: 0.9324 - loss: 1.6254 - val_accuracy: 0.5445 - val_loss: 26.7452\n",
            "Epoch 2/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 5s/step - accuracy: 0.9676 - loss: 0.9263 - val_accuracy: 0.5445 - val_loss: 22.1755\n",
            "Epoch 3/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 2s/step - accuracy: 0.9780 - loss: 0.7868 - val_accuracy: 0.5914 - val_loss: 12.8744\n",
            "Epoch 4/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 2s/step - accuracy: 0.9775 - loss: 0.7407 - val_accuracy: 0.7713 - val_loss: 6.6773\n",
            "Epoch 5/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 2s/step - accuracy: 0.9776 - loss: 0.6766 - val_accuracy: 0.8325 - val_loss: 3.8048\n",
            "Epoch 6/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 5s/step - accuracy: 0.9770 - loss: 0.6196 - val_accuracy: 0.9139 - val_loss: 1.9598\n",
            "Epoch 7/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 2s/step - accuracy: 0.9839 - loss: 0.5685 - val_accuracy: 0.9244 - val_loss: 1.7238\n",
            "Epoch 8/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 2s/step - accuracy: 0.9779 - loss: 0.5708 - val_accuracy: 0.9081 - val_loss: 1.7194\n",
            "Epoch 9/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 2s/step - accuracy: 0.9808 - loss: 0.5533 - val_accuracy: 0.9368 - val_loss: 1.4311\n",
            "Epoch 10/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 2s/step - accuracy: 0.9812 - loss: 0.5804 - val_accuracy: 0.9569 - val_loss: 1.6827\n",
            "Epoch 11/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 2s/step - accuracy: 0.9683 - loss: 0.7747 - val_accuracy: 0.9541 - val_loss: 1.4498\n",
            "Epoch 12/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 2s/step - accuracy: 0.9710 - loss: 0.7069 - val_accuracy: 0.9254 - val_loss: 1.5356\n",
            "Epoch 13/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 2s/step - accuracy: 0.9847 - loss: 0.6163 - val_accuracy: 0.9531 - val_loss: 1.0070\n",
            "Epoch 14/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.9869 - loss: 0.5401 - val_accuracy: 0.9550 - val_loss: 0.9148\n",
            "Epoch 15/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.9824 - loss: 0.5160 - val_accuracy: 0.9569 - val_loss: 0.9668\n",
            "Epoch 16/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 5s/step - accuracy: 0.9816 - loss: 0.5952 - val_accuracy: 0.9273 - val_loss: 1.6351\n",
            "Epoch 17/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 2s/step - accuracy: 0.9834 - loss: 0.5550 - val_accuracy: 0.9435 - val_loss: 1.3281\n",
            "Epoch 18/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 2s/step - accuracy: 0.9822 - loss: 0.5217 - val_accuracy: 0.9656 - val_loss: 0.9348\n",
            "Epoch 19/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.9891 - loss: 0.4564 - val_accuracy: 0.9483 - val_loss: 1.2190\n",
            "Epoch 20/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 3s/step - accuracy: 0.9814 - loss: 0.4725 - val_accuracy: 0.9378 - val_loss: 1.0284\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.9843 - loss: 0.6315\n",
            "Fold 1 Validation Accuracy: 95.50%\n",
            "\n",
            "Training Fold 2/5...\n",
            "\n",
            "Found 4177 validated image filenames belonging to 2 classes.\n",
            "Found 1045 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 2s/step - accuracy: 0.9090 - loss: 2.6395 - val_accuracy: 0.6919 - val_loss: 2.4172\n",
            "Epoch 2/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.9679 - loss: 0.8916 - val_accuracy: 0.7426 - val_loss: 3.5553\n",
            "Epoch 3/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 2s/step - accuracy: 0.9725 - loss: 0.7150 - val_accuracy: 0.7656 - val_loss: 5.1017\n",
            "Epoch 4/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m938s\u001b[0m 7s/step - accuracy: 0.9726 - loss: 0.6940 - val_accuracy: 0.8402 - val_loss: 4.2115\n",
            "Epoch 5/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 2s/step - accuracy: 0.9760 - loss: 0.6415 - val_accuracy: 0.8469 - val_loss: 3.5209\n",
            "Epoch 6/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 4s/step - accuracy: 0.9811 - loss: 0.6133 - val_accuracy: 0.8880 - val_loss: 2.4216\n",
            "Epoch 7/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.9764 - loss: 0.5698 - val_accuracy: 0.9081 - val_loss: 1.9503\n",
            "Epoch 8/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.9763 - loss: 0.5543 - val_accuracy: 0.9158 - val_loss: 2.0338\n",
            "Epoch 9/25\n",
            "\u001b[1m 93/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m4:19\u001b[0m 7s/step - accuracy: 0.9772 - loss: 0.5500"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import uuid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def detect_duplicates(folder):\n",
        "    \n",
        "    seen = {}\n",
        "    duplicates = {}\n",
        "    for cls in os.listdir(folder):\n",
        "        class_path = os.path.join(folder, cls)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        duplicates[cls] = []\n",
        "        for img_name in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img_hash = hash(img.tobytes())\n",
        "                if img_hash in seen:\n",
        "                    duplicates[cls].append(img_path)\n",
        "                else:\n",
        "                    seen[img_hash] = img_path\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {e}\")\n",
        "    return duplicates\n",
        "\n",
        "def augment_image(image):\n",
        "    return image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "def augment_duplicates(folder, duplicates_per_class, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    for cls, duplicate_files in duplicates_per_class.items():\n",
        "        class_output_path = os.path.join(output_folder, cls)\n",
        "        os.makedirs(class_output_path, exist_ok=True)\n",
        "        for filepath in duplicate_files:\n",
        "            try:\n",
        "                with Image.open(filepath) as img:\n",
        "                    augmented_img = augment_image(img)\n",
        "                    new_filename = f\"{os.path.splitext(os.path.basename(filepath))[0]}_{uuid.uuid4().hex[:6]}.png\"\n",
        "                    new_filepath = os.path.join(class_output_path, new_filename)\n",
        "                    augmented_img.save(new_filepath)\n",
        "            except Exception as e:\n",
        "                print(f\"Error augmenting {filepath}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next time remeber to print training , testing and validation accuracies, when fitting use history = model.fit....\n",
        "Reduce Model Complexity: Try reducing the number of filters or layers.\n",
        "Early Stopping: Reduce patience to stop training before overfitting occurs.\n",
        "and trainig and tesing curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Fold 1/3...\n",
            "\n",
            "Found 3481 validated image filenames belonging to 2 classes.\n",
            "Found 1741 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9208 - loss: 3.0141"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 4s/step - accuracy: 0.9211 - loss: 3.0100 - val_accuracy: 0.5445 - val_loss: 50.2085\n",
            "Epoch 2/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 4s/step - accuracy: 0.9532 - loss: 2.2498 - val_accuracy: 0.5445 - val_loss: 44.5896\n",
            "Epoch 3/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9686 - loss: 1.9827"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 4s/step - accuracy: 0.9686 - loss: 1.9817 - val_accuracy: 0.5589 - val_loss: 23.8171\n",
            "Epoch 4/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9703 - loss: 1.8500"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 4s/step - accuracy: 0.9703 - loss: 1.8491 - val_accuracy: 0.6479 - val_loss: 22.9004\n",
            "Epoch 5/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9642 - loss: 1.8123"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 4s/step - accuracy: 0.9643 - loss: 1.8109 - val_accuracy: 0.8909 - val_loss: 4.0235\n",
            "Epoch 6/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9808 - loss: 1.4421"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 4s/step - accuracy: 0.9808 - loss: 1.4418 - val_accuracy: 0.9150 - val_loss: 5.2394\n",
            "Epoch 7/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9825 - loss: 1.3709"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 4s/step - accuracy: 0.9825 - loss: 1.3706 - val_accuracy: 0.9696 - val_loss: 2.0538\n",
            "Epoch 8/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 4s/step - accuracy: 0.9806 - loss: 1.2586 - val_accuracy: 0.9563 - val_loss: 2.3374\n",
            "Epoch 9/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9757 - loss: 1.1993"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 4s/step - accuracy: 0.9757 - loss: 1.1996 - val_accuracy: 0.9793 - val_loss: 1.6569\n",
            "Epoch 10/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 5s/step - accuracy: 0.9787 - loss: 1.1861 - val_accuracy: 0.9742 - val_loss: 1.6485\n",
            "Epoch 11/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 5s/step - accuracy: 0.9787 - loss: 1.1588 - val_accuracy: 0.9736 - val_loss: 1.4696\n",
            "Epoch 12/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 4s/step - accuracy: 0.9858 - loss: 1.0853 - val_accuracy: 0.9638 - val_loss: 1.5836\n",
            "Epoch 13/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 4s/step - accuracy: 0.9782 - loss: 1.0154 - val_accuracy: 0.9742 - val_loss: 1.4151\n",
            "Epoch 14/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 4s/step - accuracy: 0.9807 - loss: 0.9992 - val_accuracy: 0.9500 - val_loss: 2.0057\n",
            "Epoch 15/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9816 - loss: 0.9783"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 4s/step - accuracy: 0.9816 - loss: 0.9781 - val_accuracy: 0.9805 - val_loss: 1.2149\n",
            "Epoch 16/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 4s/step - accuracy: 0.9833 - loss: 0.8815 - val_accuracy: 0.9765 - val_loss: 1.2491\n",
            "Epoch 17/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 4s/step - accuracy: 0.9790 - loss: 0.8762 - val_accuracy: 0.9799 - val_loss: 1.0367\n",
            "Epoch 18/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 4s/step - accuracy: 0.9803 - loss: 0.8632 - val_accuracy: 0.9615 - val_loss: 1.3185\n",
            "Epoch 19/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 4s/step - accuracy: 0.9792 - loss: 0.8680 - val_accuracy: 0.6623 - val_loss: 4.4782\n",
            "Epoch 20/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9675 - loss: 1.2353"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 4s/step - accuracy: 0.9675 - loss: 1.2358 - val_accuracy: 0.9851 - val_loss: 1.6433\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1s/step - accuracy: 0.9924 - loss: 0.8970\n",
            "Fold 1 Validation Accuracy: 97.99%\n",
            "\n",
            "Training Fold 2/3...\n",
            "\n",
            "Found 3481 validated image filenames belonging to 2 classes.\n",
            "Found 1741 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9245 - loss: 3.0907"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 4s/step - accuracy: 0.9247 - loss: 3.0863 - val_accuracy: 0.5439 - val_loss: 31.0364\n",
            "Epoch 2/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m814s\u001b[0m 7s/step - accuracy: 0.9682 - loss: 1.8253 - val_accuracy: 0.5439 - val_loss: 39.0930\n",
            "Epoch 3/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.9747 - loss: 1.6873"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m788s\u001b[0m 7s/step - accuracy: 0.9746 - loss: 1.6879 - val_accuracy: 0.5485 - val_loss: 37.0861\n",
            "Epoch 4/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9708 - loss: 1.7950"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 6s/step - accuracy: 0.9708 - loss: 1.7948 - val_accuracy: 0.6904 - val_loss: 14.7714\n",
            "Epoch 5/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.9766 - loss: 1.7369"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m781s\u001b[0m 7s/step - accuracy: 0.9766 - loss: 1.7362 - val_accuracy: 0.8334 - val_loss: 10.6510\n",
            "Epoch 6/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9738 - loss: 1.7707"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 4s/step - accuracy: 0.9738 - loss: 1.7699 - val_accuracy: 0.9495 - val_loss: 2.9009\n",
            "Epoch 7/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 4s/step - accuracy: 0.9749 - loss: 1.5567 - val_accuracy: 0.9282 - val_loss: 4.4378\n",
            "Epoch 8/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 4s/step - accuracy: 0.9841 - loss: 1.3457 - val_accuracy: 0.9385 - val_loss: 3.4215\n",
            "Epoch 9/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9815 - loss: 1.3839"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 4s/step - accuracy: 0.9814 - loss: 1.3837 - val_accuracy: 0.9540 - val_loss: 2.4120\n",
            "Epoch 10/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9864 - loss: 1.2951"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 5s/step - accuracy: 0.9864 - loss: 1.2947 - val_accuracy: 0.9604 - val_loss: 1.7103\n",
            "Epoch 11/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 5s/step - accuracy: 0.9875 - loss: 1.1492 - val_accuracy: 0.9558 - val_loss: 2.0964\n",
            "Epoch 12/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9873 - loss: 1.0774"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 4s/step - accuracy: 0.9873 - loss: 1.0774 - val_accuracy: 0.9678 - val_loss: 1.5329\n",
            "Epoch 13/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 4s/step - accuracy: 0.9856 - loss: 1.0321 - val_accuracy: 0.9540 - val_loss: 1.7486\n",
            "Epoch 14/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 4s/step - accuracy: 0.9879 - loss: 0.9643 - val_accuracy: 0.9495 - val_loss: 1.7137\n",
            "Epoch 15/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 4s/step - accuracy: 0.9848 - loss: 0.9308 - val_accuracy: 0.9489 - val_loss: 1.8907\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 1s/step - accuracy: 0.9889 - loss: 1.1853\n",
            "Fold 2 Validation Accuracy: 96.78%\n",
            "\n",
            "Training Fold 3/3...\n",
            "\n",
            "Found 3482 validated image filenames belonging to 2 classes.\n",
            "Found 1740 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9106 - loss: 3.9483"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 4s/step - accuracy: 0.9109 - loss: 3.9430 - val_accuracy: 0.5443 - val_loss: 27.6377\n",
            "Epoch 2/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 4s/step - accuracy: 0.9671 - loss: 1.7858 - val_accuracy: 0.5443 - val_loss: 30.9467\n",
            "Epoch 3/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9759 - loss: 1.5642"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 4s/step - accuracy: 0.9758 - loss: 1.5647 - val_accuracy: 0.5741 - val_loss: 15.3393\n",
            "Epoch 4/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9672 - loss: 1.5496"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 4s/step - accuracy: 0.9673 - loss: 1.5489 - val_accuracy: 0.7586 - val_loss: 8.7484\n",
            "Epoch 5/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9777 - loss: 1.3549"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 4s/step - accuracy: 0.9777 - loss: 1.3552 - val_accuracy: 0.9006 - val_loss: 4.2775\n",
            "Epoch 6/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9805 - loss: 1.2472"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 4s/step - accuracy: 0.9805 - loss: 1.2473 - val_accuracy: 0.9247 - val_loss: 4.1805\n",
            "Epoch 7/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9773 - loss: 1.1780"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 4s/step - accuracy: 0.9773 - loss: 1.1778 - val_accuracy: 0.9391 - val_loss: 3.3243\n",
            "Epoch 8/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9844 - loss: 1.1052"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 4s/step - accuracy: 0.9844 - loss: 1.1051 - val_accuracy: 0.9477 - val_loss: 2.9299\n",
            "Epoch 9/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9840 - loss: 1.0469"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 4s/step - accuracy: 0.9840 - loss: 1.0468 - val_accuracy: 0.9546 - val_loss: 2.5376\n",
            "Epoch 10/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 4s/step - accuracy: 0.9754 - loss: 1.1059 - val_accuracy: 0.9471 - val_loss: 2.7728\n",
            "Epoch 11/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 4s/step - accuracy: 0.9682 - loss: 1.1749 - val_accuracy: 0.9224 - val_loss: 3.4382\n",
            "Epoch 12/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 4s/step - accuracy: 0.9816 - loss: 1.0418 - val_accuracy: 0.9466 - val_loss: 2.3700\n",
            "Epoch 13/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9759 - loss: 1.0981"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 4s/step - accuracy: 0.9758 - loss: 1.0984 - val_accuracy: 0.9718 - val_loss: 1.5386\n",
            "Epoch 14/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 4s/step - accuracy: 0.9831 - loss: 1.0326 - val_accuracy: 0.9655 - val_loss: 1.9559\n",
            "Epoch 15/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 4s/step - accuracy: 0.9813 - loss: 0.9357 - val_accuracy: 0.9592 - val_loss: 2.2907\n",
            "Epoch 16/20\n",
            "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 4s/step - accuracy: 0.9828 - loss: 0.8894 - val_accuracy: 0.9511 - val_loss: 2.6182\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 1s/step - accuracy: 0.9882 - loss: 1.2027\n",
            "Fold 3 Validation Accuracy: 97.18%\n",
            "\n",
            "Final Cross-Validated Accuracy: 97.32%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 748 images belonging to 2 classes.\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Non-Fundus       0.97      1.00      0.98       407\n",
            "      Fundus       1.00      0.96      0.98       341\n",
            "\n",
            "    accuracy                           0.98       748\n",
            "   macro avg       0.98      0.98      0.98       748\n",
            "weighted avg       0.98      0.98      0.98       748\n",
            "\n",
            "\n",
            "Model saved at: D:/saved_model_fundus.h5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Paths\n",
        "dataset_path = \"D:/Datasets_Retina\"\n",
        "final_dataset_path = \"D:/Datasets_Final\"\n",
        "model_save_path = \"D:/saved_model_fundus.h5\"\n",
        "\n",
        "# Image Parameters\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "K_FOLDS = 3  \n",
        "\n",
        "# Function for Image Preprocessing\n",
        "def preprocess_image(img_path):\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")  # Ensure RGB format\n",
        "        img = img.resize(IMG_SIZE)  # Resize to target size\n",
        "        img = np.array(img) / 255.0  # Normalize pixel values\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load Data\n",
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "for cls in os.listdir(os.path.join(final_dataset_path, \"train\")):\n",
        "    class_path = os.path.join(final_dataset_path, \"train\", cls)\n",
        "    label = 1 if cls == \"Fundus\" else 0  # Binary classification\n",
        "    for img_name in os.listdir(class_path):\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "        processed_img = preprocess_image(img_path)\n",
        "        if processed_img is not None:\n",
        "            all_images.append(img_path)\n",
        "            all_labels.append(label)\n",
        "\n",
        "all_images = np.array(all_images)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "fold_accuracies = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(all_images, all_labels)):\n",
        "    print(f\"\\nTraining Fold {fold + 1}/{K_FOLDS}...\\n\")\n",
        "\n",
        "    train_images, val_images = all_images[train_idx], all_images[val_idx]\n",
        "    train_labels, val_labels = all_labels[train_idx], all_labels[val_idx]\n",
        "\n",
        "    train_df = pd.DataFrame({\"filename\": train_images, \"class\": [ \"Fundus\" if lbl == 1 else \"Non-Fundus\" for lbl in train_labels]})\n",
        "    val_df = pd.DataFrame({\"filename\": val_images, \"class\": [ \"Fundus\" if lbl == 1 else \"Non-Fundus\" for lbl in val_labels]})\n",
        "\n",
        "\n",
        "    # Data Augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=25,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.3,\n",
        "        shear_range=0.2,\n",
        "        brightness_range=[0.7, 1.3]\n",
        "    )\n",
        "\n",
        "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_dataframe(\n",
        "        train_df, x_col=\"filename\", y_col=\"class\",\n",
        "        target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "        class_mode=\"binary\", shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow_from_dataframe(\n",
        "        val_df, x_col=\"filename\", y_col=\"class\",\n",
        "        target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "        class_mode=\"binary\", shuffle=False\n",
        "    )\n",
        "\n",
        "    # Optimized CNN Model\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=(224, 224, 3)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0002), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Callbacks for Early Stopping and Best Model Saving\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    checkpoint = ModelCheckpoint(model_save_path, monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "\n",
        "    # Train Model\n",
        "    model.fit(train_generator, validation_data=val_generator, epochs=20, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "    # Evaluate Model\n",
        "    val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "    fold_accuracies.append(val_accuracy * 100)\n",
        "    print(f\"Fold {fold+1} Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Compute Final Cross-Validation Accuracy\n",
        "final_accuracy = np.mean(fold_accuracies)\n",
        "print(f\"\\nFinal Cross-Validated Accuracy: {final_accuracy:.2f}%\")\n",
        "best_model = tf.keras.models.load_model(model_save_path)\n",
        "\n",
        "# Test Data Evaluation\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "    os.path.join(final_dataset_path, \"test\"),\n",
        "    target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode=\"binary\", shuffle=False\n",
        ")\n",
        "\n",
        "y_true = test_generator.classes\n",
        "y_pred = (best_model.predict(test_generator) > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nFinal Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Non-Fundus\", \"Fundus\"]))\n",
        "\n",
        "best_model.save(model_save_path)\n",
        "print(f\"\\nModel saved at: {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9911 - loss: 1.0744\n",
            "Test Accuracy: 98.13%\n",
            "Test Loss: 1.2224\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"D:/saved_model_fundus.h5\")\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Fundus\", \"Fundus\"], yticklabels=[\"Non-Fundus\", \"Fundus\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded in 0.87 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_path = \"D:/saved_model_fundus.h5\"\n",
        "\n",
        "start_time = time.time()\n",
        "model = load_model(model_path)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Model loaded in {end_time - start_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "model_path = \"D:/saved_model_fundus.h5\"\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to preprocess the image\n",
        "def preprocess_single_image(img_path, target_size=(224, 224)):\n",
        "    img = image.load_img(img_path, target_size=target_size)  # Load image\n",
        "    img_array = image.img_to_array(img)  # Convert to array\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array = img_array / 255.0  # Normalize (same as training)\n",
        "    return img_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Prediction: [[1.233044e-19]]\n",
            "Model predicts: Fundus\n"
          ]
        }
      ],
      "source": [
        "img_path = r\"D:\\Datasets_Final\\test\\Fundus\\N3.jpg\"\n",
        "img = image.load_img(img_path, target_size=(224, 224))  \n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0) / 255.0  \n",
        "\n",
        "prediction = model.predict(img_array)\n",
        "\n",
        "# Check output\n",
        "print(\"Prediction:\", prediction)\n",
        "\n",
        "# Interpret the prediction based on your labeling (e.g., 0 = fundus, 1 = non-fundus)\n",
        "if prediction[0] > 0.5:\n",
        "    print(\"Model predicts: Non-Fundus\")\n",
        "else:\n",
        "    print(\"Model predicts: Fundus\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Predicted Class: Non-Fundus (Confidence: 0.0000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19436\\4066659428.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  predicted_class = class_labels[int(prediction[0] > 0.5)]\n"
          ]
        }
      ],
      "source": [
        "test_image_path = \"C:\\\\Users\\\\User\\\\Downloads\\\\download.jpeg\"\n",
        "input_image = preprocess_single_image(test_image_path)\n",
        "prediction = model.predict(input_image)\n",
        "\n",
        "# Interpret Prediction\n",
        "class_labels = [\"Non-Fundus\", \"Fundus\"]\n",
        "predicted_class = class_labels[int(prediction[0] > 0.5)]\n",
        "print(f\"Predicted Class: {predicted_class} (Confidence: {prediction[0][0]:.4f})\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPidfDTkuK6fCqJFTRmVsB5",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Romal27/DSGP-24-Retina94/blob/Validating-the-input/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5222 images belonging to 2 classes.\n",
            "Found 1492 images belonging to 2 classes.\n",
            "Found 748 images belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 3s/step - accuracy: 0.9392 - loss: 1.7771 - val_accuracy: 0.5442 - val_loss: 19.9984\n",
            "Epoch 2/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 3s/step - accuracy: 0.9706 - loss: 0.9057 - val_accuracy: 0.5509 - val_loss: 18.9203\n",
            "Epoch 3/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 2s/step - accuracy: 0.9772 - loss: 0.7324 - val_accuracy: 0.7882 - val_loss: 5.6246\n",
            "Epoch 4/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 2s/step - accuracy: 0.9808 - loss: 0.6411 - val_accuracy: 0.9155 - val_loss: 2.9250\n",
            "Epoch 5/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 2s/step - accuracy: 0.9848 - loss: 0.5825 - val_accuracy: 0.9350 - val_loss: 2.2543\n",
            "Epoch 6/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 2s/step - accuracy: 0.9776 - loss: 0.6140 - val_accuracy: 0.9015 - val_loss: 3.1671\n",
            "Epoch 7/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 2s/step - accuracy: 0.9800 - loss: 0.6211 - val_accuracy: 0.9403 - val_loss: 1.7575\n",
            "Epoch 8/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 2s/step - accuracy: 0.9742 - loss: 0.6756 - val_accuracy: 0.9055 - val_loss: 2.6261\n",
            "Epoch 9/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 3s/step - accuracy: 0.9862 - loss: 0.5478 - val_accuracy: 0.9350 - val_loss: 1.4005\n",
            "Epoch 10/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 4s/step - accuracy: 0.9829 - loss: 0.5213 - val_accuracy: 0.9491 - val_loss: 1.4401\n",
            "Epoch 11/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 4s/step - accuracy: 0.9835 - loss: 0.5318 - val_accuracy: 0.9705 - val_loss: 1.0213\n",
            "Epoch 12/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 4s/step - accuracy: 0.9726 - loss: 0.6922 - val_accuracy: 0.9651 - val_loss: 1.0392\n",
            "Epoch 13/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m677s\u001b[0m 4s/step - accuracy: 0.9741 - loss: 0.6336 - val_accuracy: 0.9558 - val_loss: 0.9952\n",
            "Epoch 14/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 4s/step - accuracy: 0.9866 - loss: 0.5180 - val_accuracy: 0.9517 - val_loss: 1.3759\n",
            "Epoch 15/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 4s/step - accuracy: 0.9792 - loss: 0.5432 - val_accuracy: 0.9504 - val_loss: 1.0227\n",
            "Epoch 16/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 3s/step - accuracy: 0.9761 - loss: 0.5981 - val_accuracy: 0.9578 - val_loss: 1.8112\n",
            "Epoch 17/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 3s/step - accuracy: 0.9831 - loss: 0.5688 - val_accuracy: 0.9504 - val_loss: 1.2425\n",
            "Epoch 18/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 3s/step - accuracy: 0.9815 - loss: 0.5177 - val_accuracy: 0.9638 - val_loss: 0.7684\n",
            "Epoch 19/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 3s/step - accuracy: 0.9787 - loss: 0.5021 - val_accuracy: 0.9336 - val_loss: 1.2879\n",
            "Epoch 20/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 3s/step - accuracy: 0.9795 - loss: 0.5598 - val_accuracy: 0.9645 - val_loss: 1.0502\n",
            "Epoch 21/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 3s/step - accuracy: 0.9867 - loss: 0.5104 - val_accuracy: 0.9517 - val_loss: 1.4499\n",
            "Epoch 22/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 2s/step - accuracy: 0.9838 - loss: 0.5445 - val_accuracy: 0.9786 - val_loss: 0.9092\n",
            "Epoch 23/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 2s/step - accuracy: 0.9788 - loss: 0.5282 - val_accuracy: 0.9584 - val_loss: 1.3150\n",
            "Epoch 24/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 2s/step - accuracy: 0.9861 - loss: 0.5279 - val_accuracy: 0.9786 - val_loss: 0.6780\n",
            "Epoch 25/25\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 2s/step - accuracy: 0.9871 - loss: 0.4613 - val_accuracy: 0.9826 - val_loss: 0.6045\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9928 - loss: 0.4598\n",
            "\n",
            "Validation Accuracy: 98.06%\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.9911 - loss: 0.4559\n",
            "\n",
            "Test Accuracy: 97.86%\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Non-Fundus       0.97      1.00      0.98       407\n",
            "      Fundus       1.00      0.96      0.98       341\n",
            "\n",
            "    accuracy                           0.98       748\n",
            "   macro avg       0.98      0.98      0.98       748\n",
            "weighted avg       0.98      0.98      0.98       748\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import uuid\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def detect_duplicates(folder):\n",
        "    seen = {}\n",
        "    duplicates = {}\n",
        "    for cls in os.listdir(folder):\n",
        "        class_path = os.path.join(folder, cls)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        duplicates[cls] = []\n",
        "        for img_name in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img_hash = hash(img.tobytes())\n",
        "                if img_hash in seen:\n",
        "                    duplicates[cls].append(img_path)\n",
        "                else:\n",
        "                    seen[img_hash] = img_path\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {e}\")\n",
        "    return duplicates\n",
        "\n",
        "\n",
        "def augment_image(image):\n",
        "    return image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "def augment_duplicates(folder, duplicates_per_class, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    for cls, duplicate_files in duplicates_per_class.items():\n",
        "        class_output_path = os.path.join(output_folder, cls)\n",
        "        os.makedirs(class_output_path, exist_ok=True)\n",
        "        for filepath in duplicate_files:\n",
        "            try:\n",
        "                with Image.open(filepath) as img:\n",
        "                    augmented_img = augment_image(img)\n",
        "                    new_filename = f\"{os.path.splitext(os.path.basename(filepath))[0]}_{uuid.uuid4().hex[:6]}.png\"\n",
        "                    new_filepath = os.path.join(class_output_path, new_filename)\n",
        "                    augmented_img.save(new_filepath)\n",
        "            except Exception as e:\n",
        "                print(f\"Error augmenting {filepath}: {e}\")\n",
        "                \n",
        "def split_data(source_dir, output_dir, train_ratio=0.7, val_ratio=0.2):\n",
        "    test_ratio = 1 - (train_ratio + val_ratio)\n",
        "    for cls in os.listdir(source_dir):\n",
        "        class_path = os.path.join(source_dir, cls)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        images = os.listdir(class_path)\n",
        "        train, temp = train_test_split(images, test_size=(1 - train_ratio), stratify=[cls]*len(images), random_state=42)\n",
        "        val, test = train_test_split(temp, test_size=(test_ratio / (val_ratio + test_ratio)), stratify=[cls]*len(temp), random_state=42)\n",
        "        for subset, subset_images in zip([\"train\", \"val\", \"test\"], [train, val, test]):\n",
        "            subset_path = os.path.join(output_dir, subset, cls)\n",
        "            os.makedirs(subset_path, exist_ok=True)\n",
        "            for img_name in subset_images:\n",
        "                shutil.copy(os.path.join(class_path, img_name), os.path.join(subset_path, img_name))\n",
        "\n",
        "# Define Paths\n",
        "dataset_path = \"D:/Datasets_Retina\"\n",
        "augmented_dataset_path = \"D:/Datasets_Retina_Augmented\"\n",
        "combined_dataset_path = \"D:/Datasets_Combined\"\n",
        "final_dataset_path = \"D:/Datasets_Final\"\n",
        "\n",
        "# Handle duplicates\n",
        "duplicates_per_class = detect_duplicates(dataset_path)\n",
        "augment_duplicates(dataset_path, duplicates_per_class, augmented_dataset_path)\n",
        "\n",
        "# Combine original and augmented datasets\n",
        "shutil.copytree(dataset_path, combined_dataset_path, dirs_exist_ok=True)\n",
        "shutil.copytree(augmented_dataset_path, combined_dataset_path, dirs_exist_ok=True)\n",
        "\n",
        "# Split dataset properly to prevent data leakage\n",
        "split_data(combined_dataset_path, final_dataset_path)\n",
        "\n",
        "# Image Processing Parameters\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data Augmentation for training set\n",
        "data_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.2,\n",
        "    brightness_range=[0.7, 1.3]\n",
        ")\n",
        "\n",
        "# Load Data\n",
        "train_generator = data_gen.flow_from_directory(os.path.join(final_dataset_path, \"train\"), target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"binary\")\n",
        "val_generator = data_gen.flow_from_directory(os.path.join(final_dataset_path, \"val\"), target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"binary\", shuffle=False)\n",
        "test_generator = data_gen.flow_from_directory(os.path.join(final_dataset_path, \"test\"), target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=\"binary\", shuffle=False)\n",
        "\n",
        "# Improved CNN Model with Regularization\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=(224, 224, 3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.5),\n",
        "    \n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile Model with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "\n",
        "# Train Model\n",
        "model.fit(train_generator, validation_data=val_generator, epochs=25, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate Model\n",
        "val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "print(f\"\\nValidation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predictions\n",
        "y_true = test_generator.classes\n",
        "y_pred = (model.predict(test_generator) > 0.5).astype(int)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Non-Fundus\", \"Fundus\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Fold 1/5...\n",
            "\n",
            "Found 4177 validated image filenames belonging to 2 classes.\n",
            "Found 1045 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 5s/step - accuracy: 0.9324 - loss: 1.6254 - val_accuracy: 0.5445 - val_loss: 26.7452\n",
            "Epoch 2/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 5s/step - accuracy: 0.9676 - loss: 0.9263 - val_accuracy: 0.5445 - val_loss: 22.1755\n",
            "Epoch 3/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 2s/step - accuracy: 0.9780 - loss: 0.7868 - val_accuracy: 0.5914 - val_loss: 12.8744\n",
            "Epoch 4/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 2s/step - accuracy: 0.9775 - loss: 0.7407 - val_accuracy: 0.7713 - val_loss: 6.6773\n",
            "Epoch 5/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 2s/step - accuracy: 0.9776 - loss: 0.6766 - val_accuracy: 0.8325 - val_loss: 3.8048\n",
            "Epoch 6/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 5s/step - accuracy: 0.9770 - loss: 0.6196 - val_accuracy: 0.9139 - val_loss: 1.9598\n",
            "Epoch 7/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 2s/step - accuracy: 0.9839 - loss: 0.5685 - val_accuracy: 0.9244 - val_loss: 1.7238\n",
            "Epoch 8/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 2s/step - accuracy: 0.9779 - loss: 0.5708 - val_accuracy: 0.9081 - val_loss: 1.7194\n",
            "Epoch 9/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 2s/step - accuracy: 0.9808 - loss: 0.5533 - val_accuracy: 0.9368 - val_loss: 1.4311\n",
            "Epoch 10/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 2s/step - accuracy: 0.9812 - loss: 0.5804 - val_accuracy: 0.9569 - val_loss: 1.6827\n",
            "Epoch 11/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 2s/step - accuracy: 0.9683 - loss: 0.7747 - val_accuracy: 0.9541 - val_loss: 1.4498\n",
            "Epoch 12/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 2s/step - accuracy: 0.9710 - loss: 0.7069 - val_accuracy: 0.9254 - val_loss: 1.5356\n",
            "Epoch 13/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 2s/step - accuracy: 0.9847 - loss: 0.6163 - val_accuracy: 0.9531 - val_loss: 1.0070\n",
            "Epoch 14/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.9869 - loss: 0.5401 - val_accuracy: 0.9550 - val_loss: 0.9148\n",
            "Epoch 15/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.9824 - loss: 0.5160 - val_accuracy: 0.9569 - val_loss: 0.9668\n",
            "Epoch 16/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 5s/step - accuracy: 0.9816 - loss: 0.5952 - val_accuracy: 0.9273 - val_loss: 1.6351\n",
            "Epoch 17/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 2s/step - accuracy: 0.9834 - loss: 0.5550 - val_accuracy: 0.9435 - val_loss: 1.3281\n",
            "Epoch 18/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 2s/step - accuracy: 0.9822 - loss: 0.5217 - val_accuracy: 0.9656 - val_loss: 0.9348\n",
            "Epoch 19/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.9891 - loss: 0.4564 - val_accuracy: 0.9483 - val_loss: 1.2190\n",
            "Epoch 20/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 3s/step - accuracy: 0.9814 - loss: 0.4725 - val_accuracy: 0.9378 - val_loss: 1.0284\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.9843 - loss: 0.6315\n",
            "Fold 1 Validation Accuracy: 95.50%\n",
            "\n",
            "Training Fold 2/5...\n",
            "\n",
            "Found 4177 validated image filenames belonging to 2 classes.\n",
            "Found 1045 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 2s/step - accuracy: 0.9090 - loss: 2.6395 - val_accuracy: 0.6919 - val_loss: 2.4172\n",
            "Epoch 2/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.9679 - loss: 0.8916 - val_accuracy: 0.7426 - val_loss: 3.5553\n",
            "Epoch 3/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 2s/step - accuracy: 0.9725 - loss: 0.7150 - val_accuracy: 0.7656 - val_loss: 5.1017\n",
            "Epoch 4/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m938s\u001b[0m 7s/step - accuracy: 0.9726 - loss: 0.6940 - val_accuracy: 0.8402 - val_loss: 4.2115\n",
            "Epoch 5/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 2s/step - accuracy: 0.9760 - loss: 0.6415 - val_accuracy: 0.8469 - val_loss: 3.5209\n",
            "Epoch 6/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 4s/step - accuracy: 0.9811 - loss: 0.6133 - val_accuracy: 0.8880 - val_loss: 2.4216\n",
            "Epoch 7/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.9764 - loss: 0.5698 - val_accuracy: 0.9081 - val_loss: 1.9503\n",
            "Epoch 8/25\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.9763 - loss: 0.5543 - val_accuracy: 0.9158 - val_loss: 2.0338\n",
            "Epoch 9/25\n",
            "\u001b[1m 93/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m4:19\u001b[0m 7s/step - accuracy: 0.9772 - loss: 0.5500"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import uuid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def detect_duplicates(folder):\n",
        "    \"\"\"Detect duplicate images in dataset\"\"\"\n",
        "    seen = {}\n",
        "    duplicates = {}\n",
        "    for cls in os.listdir(folder):\n",
        "        class_path = os.path.join(folder, cls)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        duplicates[cls] = []\n",
        "        for img_name in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img_hash = hash(img.tobytes())\n",
        "                if img_hash in seen:\n",
        "                    duplicates[cls].append(img_path)\n",
        "                else:\n",
        "                    seen[img_hash] = img_path\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {e}\")\n",
        "    return duplicates\n",
        "\n",
        "def augment_image(image):\n",
        "    return image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "def augment_duplicates(folder, duplicates_per_class, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    for cls, duplicate_files in duplicates_per_class.items():\n",
        "        class_output_path = os.path.join(output_folder, cls)\n",
        "        os.makedirs(class_output_path, exist_ok=True)\n",
        "        for filepath in duplicate_files:\n",
        "            try:\n",
        "                with Image.open(filepath) as img:\n",
        "                    augmented_img = augment_image(img)\n",
        "                    new_filename = f\"{os.path.splitext(os.path.basename(filepath))[0]}_{uuid.uuid4().hex[:6]}.png\"\n",
        "                    new_filepath = os.path.join(class_output_path, new_filename)\n",
        "                    augmented_img.save(new_filepath)\n",
        "            except Exception as e:\n",
        "                print(f\"Error augmenting {filepath}: {e}\")\n",
        "\n",
        "dataset_path = \"D:/Datasets_Retina\"\n",
        "final_dataset_path = \"D:/Datasets_Final\"\n",
        "\n",
        "# Image Processing\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "K_FOLDS = 3 # Number of cross-validation folds\n",
        "\n",
        "# Data Augmentation\n",
        "data_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.2,\n",
        "    brightness_range=[0.7, 1.3]\n",
        ")\n",
        "\n",
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "for cls in os.listdir(final_dataset_path + \"/train\"):\n",
        "    class_path = os.path.join(final_dataset_path, \"train\", cls)\n",
        "    label = 1 if cls == \"Fundus\" else 0  # Assuming binary classification\n",
        "    for img_name in os.listdir(class_path):\n",
        "        all_images.append(os.path.join(class_path, img_name))\n",
        "        all_labels.append(label)\n",
        "\n",
        "all_images = np.array(all_images)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "fold_accuracies = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(all_images, all_labels)):\n",
        "    print(f\"\\nTraining Fold {fold + 1}/{K_FOLDS}...\\n\")\n",
        "\n",
        "    train_images, val_images = all_images[train_idx], all_images[val_idx]\n",
        "    train_labels, val_labels = all_labels[train_idx], all_labels[val_idx]\n",
        "\n",
        "    # Create Generators\n",
        "    train_df = pd.DataFrame({\"filename\": train_images, \"class\": train_labels})\n",
        "    val_df = pd.DataFrame({\"filename\": val_images, \"class\": val_labels})\n",
        "\n",
        "  # Convert labels to string format (\"Non-Fundus\" and \"Fundus\")\n",
        "    train_df[\"class\"] = train_df[\"class\"].map({0: \"Non-Fundus\", 1: \"Fundus\"})\n",
        "    val_df[\"class\"] = val_df[\"class\"].map({0: \"Non-Fundus\", 1: \"Fundus\"})\n",
        "\n",
        "    train_generator = data_gen.flow_from_dataframe(\n",
        "        train_df, x_col=\"filename\", y_col=\"class\",\n",
        "        target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "        class_mode=\"binary\", shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = data_gen.flow_from_dataframe(\n",
        "        val_df, x_col=\"filename\", y_col=\"class\",\n",
        "        target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "        class_mode=\"binary\", shuffle=False\n",
        "    )\n",
        "\n",
        "\n",
        "    # Model Architecture\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=(224, 224, 3)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train Model with Early Stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "\n",
        "    model.fit(train_generator, validation_data=val_generator, epochs=20, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "    fold_accuracies.append(val_accuracy * 100)\n",
        "    print(f\"Fold {fold+1} Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Compute Final Cross-Validation Accuracy\n",
        "final_accuracy = np.mean(fold_accuracies)\n",
        "print(f\"\\nFinal Cross-Validated Accuracy: {final_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "test_generator = data_gen.flow_from_directory(\n",
        "    os.path.join(final_dataset_path, \"test\"),\n",
        "    target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode=\"binary\", shuffle=False\n",
        ")\n",
        "\n",
        "y_true = test_generator.classes\n",
        "y_pred = (model.predict(test_generator) > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nFinal Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Non-Fundus\", \"Fundus\"]))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPidfDTkuK6fCqJFTRmVsB5",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
